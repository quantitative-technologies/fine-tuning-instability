
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>\(\ltsp\) Regularization &#8212; Fine-Tuning Instability for Large Language Models</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "e": "\\mathrm{e}", "th": "^{\\mathrm{th}}", "s": "^{\\mathrm{src}}", "sp": "\\mathrm{SP}", "ltwo": "L^2", "ltsp": "\\ltwo-\\sp", "thetas": "\\theta^{\\mathrm{s}}", "thetan": "\\theta^{-\\mathrm{s}}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Conclusions" href="conclusion.html" />
    <link rel="prev" title="Fine-Tuning Instability" href="instability.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Fine-Tuning Instability for Large Language Models</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="instability.html">
   Fine-Tuning Instability
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   <span class="math notranslate nohighlight">
    \(\ltsp\)
   </span>
   Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Conclusions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <a href="https://quantitative-technologies.com/posts/data-science/" style="font-size:150%;">
  <b>Quantitative Technologies</b>
</a>

            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/quantitative-technologies/fine-tuning-instability"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/quantitative-technologies/fine-tuning-instability/issues/new?title=Issue%20on%20page%20%2Fl2sp_regularization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#weight-decay">
   Weight Decay
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adamw-adam-with-decoupled-weight-decay">
   AdamW: Adam with Decoupled Weight Decay
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adamwl2sp-adam-with-ltsp-weight-decay">
   AdamWL2SP: Adam with
   <span class="math notranslate nohighlight">
    \(\ltsp\)
   </span>
   Weight Decay
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#experiments">
     Experiments
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#short-runs">
       Short Runs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#long-runs">
       Long Runs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#accuracy-vs-epoch-plot">
       Accuracy vs Epoch Plot
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>\ltsp Regularization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#weight-decay">
   Weight Decay
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adamw-adam-with-decoupled-weight-decay">
   AdamW: Adam with Decoupled Weight Decay
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adamwl2sp-adam-with-ltsp-weight-decay">
   AdamWL2SP: Adam with
   <span class="math notranslate nohighlight">
    \(\ltsp\)
   </span>
   Weight Decay
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#experiments">
     Experiments
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#short-runs">
       Short Runs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#long-runs">
       Long Runs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#accuracy-vs-epoch-plot">
       Accuracy vs Epoch Plot
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell tag_remove-input docutils container">
</div>
<div class="cell tag_remove-input docutils container">
</div>
<div class="cell tag_remove-input docutils container">
</div>
<div class="cell tag_remove-input docutils container">
</div>
<section class="tex2jax_ignore mathjax_ignore" id="ltsp-regularization">
<h1><span class="math notranslate nohighlight">\(\ltsp\)</span> Regularization<a class="headerlink" href="#ltsp-regularization" title="Permalink to this headline">#</a></h1>
<p>Since the advent of BERT, various techniques have been developed or utilized to stabilize the fine-tuning process (as discussed in <a class="reference internal" href="instability.html#current-state"><span class="std std-ref">Current State of Instability for Fine-Tuning LLMs</span></a>). We investigate one of these methods, <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization, introduced in <span id="id1">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span> for the purpose of fine-tuning computer vision models, and used as one of the baseline methods in <span id="id2">[<a class="reference internal" href="references.html#id5" title="Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, and Jiebo Luo. Noise Stability Regularization for Improving BERT Fine-tuning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics, 3229–3241. July 2021. arXiv: 2107.04835. URL: https://aclanthology.org/2020.acl-main.197.pdf (visited on 2022-04-23).">HLD+21</a>]</span>. It is indicated in <span id="id3">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span> that standard <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization, which gives an inductive bias towards the origin, is inappropriate for transfer learning. Instead an inductive bias towards towards the starting point (<span class="math notranslate nohighlight">\(\sp\)</span>) from where the transfer learning (e.g. fine-tuning) begins, ostensibly preserves the knowledge from the pre-trained model. It is reported in <span id="id4">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span> that <span class="math notranslate nohighlight">\(\ltsp\)</span> always outperforms <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization, for fine-tuning deep convolutional networks on computer vision tasks, but unfortunately they do not compare with no regularization.</p>
<p>The reason we found <span class="math notranslate nohighlight">\(\ltsp\)</span> to be be of particular interest is because, according to <span id="id5">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span>, it alleviates catastrophic forgetting—whereas they claim <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization worsens catastrophic forgetting. This agreed with our observations on cases of catastrophic forgetting (not shown here) which were characterized by large shifts of the weight parameters from their pre-trained values.</p>
<p>Let <span class="math notranslate nohighlight">\(f_t(\theta)\)</span> be the <span class="math notranslate nohighlight">\(t\th\)</span>-batch loss as a function of the model weights <span class="math notranslate nohighlight">\(\theta\)</span>. Then <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization involves minimizing the <em>penalized loss</em></p>
<div class="math notranslate nohighlight" id="equation-l2-reg">
<span class="eqno">(1)<a class="headerlink" href="#equation-l2-reg" title="Permalink to this equation">#</a></span>\[
f_t^{\ltwo}(\theta)=f_t(\theta)+\frac\lambda2\|\theta\|_2^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the <em>regularization penalty</em>. It penalizes large parameters with respect to the <span class="math notranslate nohighlight">\(\ltwo\)</span> norm, pushing them towards <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>In <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization, the distance from the starting point, rather than the origin, is penalized.
However, in transfer learning the starting model architecture is usually modified for the purpose of learning the new task. For example, when fine-tuning, the last layer of the model—i.e. the model’s <em>head</em>—is typically replaced to learn some downstream task. In the case of our RTE task, the pre-trained model’s head is replaced with a classification head which decides whether or not there is an implication between the first and second sentences.</p>
<p>Let <span class="math notranslate nohighlight">\(\thetas\)</span> denote model parameters that are from the architecture (i.e. layers) shared with the source starting point model,
and let <span class="math notranslate nohighlight">\(\thetan\)</span> denote the remaining novel parameters (e.g. the fine-tuning head). <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization uses the penalized loss</p>
<div class="math notranslate nohighlight" id="equation-l2-sp-reg">
<span class="eqno">(2)<a class="headerlink" href="#equation-l2-sp-reg" title="Permalink to this equation">#</a></span>\[
f_t^{\ltsp}(\theta)=f_t(\theta)+\frac{\lambda_1}2\|\thetas-\thetas_0\|_2^2+\frac{\lambda_2}2\|\thetan\|_2^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\thetas_0\)</span> are the starting point model parameters in the shared portion of the architecture, and <span class="math notranslate nohighlight">\(\lambda_1, \lambda_2\ge 0\)</span> are the two penalty hyperparameters.</p>
<section id="weight-decay">
<h2>Weight Decay<a class="headerlink" href="#weight-decay" title="Permalink to this headline">#</a></h2>
<p>Let</p>
<div class="math notranslate nohighlight">
\[
g_t=\nabla f_t(\theta_t)
\]</div>
<p>denote the loss gradient of the <span class="math notranslate nohighlight">\(t\th\)</span>-batch. The gradient-based update with <em>exponential weight decay</em> with rate <span class="math notranslate nohighlight">\(\lambda\)</span> is given by</p>
<div class="math notranslate nohighlight" id="equation-weight-decay">
<span class="eqno">(3)<a class="headerlink" href="#equation-weight-decay" title="Permalink to this equation">#</a></span>\[
\theta_{t+1}=(1-\lambda)\theta_t-\alpha g_t,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
<p>It is well-known that (standard) stochastic gradient descent with momentum (SGD) together with weight decay <span class="math notranslate nohighlight">\(\lambda\)</span> as described in equation <a class="reference internal" href="#equation-weight-decay">(3)</a> is equivalent to SDG with <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization: That is, with no weight decay using the loss <span class="math notranslate nohighlight">\(f_t^{\ltwo}\)</span> from equation <a class="reference internal" href="#equation-l2-reg">(1)</a> with a regularization penalty of <span class="math notranslate nohighlight">\(\lambda'=\frac\lambda\alpha\)</span> (e.g. <span id="id6">[<a class="reference internal" href="references.html#id15" title="Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In Proceedings of the Seventh International Conference on Learning Representations (ICLR 2019). February 2022. URL: https://openreview.net/forum?id=Bkg6RiCqY7 (visited on 2022-08-24).">LH22</a>]</span>, Proposition 1).</p>
<p>For example, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD</a> from the PyTorch library has a weight decay parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, and uses the value <span class="math notranslate nohighlight">\(\lambda\leftarrow\alpha\lambda\)</span> for its update in equation <a class="reference internal" href="#equation-weight-decay">(3)</a>. Hence, the library’s <span class="math notranslate nohighlight">\(\lambda\)</span> parameter is in fact the <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization penalty, as indicated in its documentation.</p>
<p>In <span id="id7">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span>, where <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization is introduced, SGD is used for optimization. Equation <a class="reference internal" href="#equation-weight-decay">(3)</a> can be modified to express SGD with <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization equivalently in terms of a weight decay. Without getting into details, it is done essentially as in <a class="reference internal" href="#equation-l2sp-weight-decay">(4)</a> below.</p>
</section>
<section id="adamw-adam-with-decoupled-weight-decay">
<span id="adamw"></span><h2>AdamW: Adam with Decoupled Weight Decay<a class="headerlink" href="#adamw-adam-with-decoupled-weight-decay" title="Permalink to this headline">#</a></h2>
<p>We would like to evaluate <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization for fine-tuning ALBERT models. Since our baseline was done with the adaptive moment estimation (Adam) optimizer, this entails adding <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization to Adam rather than SGD.</p>
<p>However, the equivalence between weight decay and <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization fails for the Adam optimizer, as shown in <span id="id8">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span>, Proposition 2. Before this it had already been observed that <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization degrades the generalization performance of Adam, defeating the purpose of regularization. The reason for its failure is explained in <span id="id9">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span>. They also propose a simple solution where the gradient-based update is decoupled from the weight decay step: <span class="math notranslate nohighlight">\(\theta_{t+1}=(1-\lambda)\theta_t\)</span>. Their AdamW was demonstrated to be an effective regularizer for Adam, and is implemented in the PyTorch library as <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html"><code class="docutils literal notranslate"><span class="pre">AdamW</span></code></a>.</p>
</section>
<section id="adamwl2sp-adam-with-ltsp-weight-decay">
<h2>AdamWL2SP: Adam with <span class="math notranslate nohighlight">\(\ltsp\)</span> Weight Decay<a class="headerlink" href="#adamwl2sp-adam-with-ltsp-weight-decay" title="Permalink to this headline">#</a></h2>
<p><span id="id10">[<a class="reference internal" href="references.html#id5" title="Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, and Jiebo Luo. Noise Stability Regularization for Improving BERT Fine-tuning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics, 3229–3241. July 2021. arXiv: 2107.04835. URL: https://aclanthology.org/2020.acl-main.197.pdf (visited on 2022-04-23).">HLD+21</a>]</span> claim to have used <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization as a baseline for improving the stability of BERT fine-tuning. They simply reference <span id="id11">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span> without giving further details on their implementation. Since they explicitly state that the Adam optimizer is used in their various experiments and provide the hyperparameter settings, we assumed that they meant adding some form of <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization to the Adam optimizer. Perhaps, however, they simply used SGD with <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization as in <span id="id12">[<a class="reference internal" href="references.html#id14" title="Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In Proceedings of the 35th International Conference on Machine Learning, 2825–2834. PMLR, July 2018. URL: https://proceedings.mlr.press/v80/li18a.html (visited on 2022-07-30).">LGD18</a>]</span>.</p>
<p>In <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization, see equation <a class="reference internal" href="#equation-l2-sp-reg">(2)</a>, the distance of the weights from their initial values is being penalized. Thus the corresponding decoupled weight decay applies to the distance between current and initial weights. For the parameters <span class="math notranslate nohighlight">\(\thetas\)</span>, shared between the source and target models, the update is:</p>
<div class="math notranslate nohighlight" id="equation-l2sp-weight-decay">
<span class="eqno">(4)<a class="headerlink" href="#equation-l2sp-weight-decay" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{split}
\thetas_{t+1}&amp;=\thetas_t-\lambda_1(\thetas_t-\thetas_0)\\
&amp;=(1-\lambda_1)\thetas_t + \lambda_1\thetas_0,
\end{split}
\end{split}\]</div>
<p>while for the novel parameters this is just the <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization version, i.e. exponential weight decay: <span class="math notranslate nohighlight">\(\thetan_{t+1}=(1-\lambda_2)\thetan_t\)</span>.</p>
<section id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">#</a></h3>
<p>Our implementation of <code class="docutils literal notranslate"><span class="pre">AdamWL2SP</span></code> is available in this project’s <a class="reference external" href="https://github.com/quantitative-technologies/fine-tuning-instability">repository</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The PyTorch <code class="docutils literal notranslate"><span class="pre">AdamW</span></code> follows the <code class="docutils literal notranslate"><span class="pre">SGD</span></code> optimizer, in that the hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span> is multiplied by the learning weight <span class="math notranslate nohighlight">\(\alpha\)</span> so that the actual weight decay is <span class="math notranslate nohighlight">\(\alpha\lambda\)</span>. In view of the fact that <code class="docutils literal notranslate"><span class="pre">AdamW</span></code> decouples the weight decay from the gradient update, it seems inappropriate to multiply <span class="math notranslate nohighlight">\(\lambda\)</span> by <span class="math notranslate nohighlight">\(\alpha\text{.}\)</span> On the other hand (we did not test this), it is conceivable that the best <span class="math notranslate nohighlight">\(\lambda\)</span> is inversely proportional to <span class="math notranslate nohighlight">\(\alpha\)</span> by some vestige connection with <code class="docutils literal notranslate"><span class="pre">SGD</span></code> where there is an equivalence with <span class="math notranslate nohighlight">\(L^2\)</span> regularization with penalty <span class="math notranslate nohighlight">\(\alpha\lambda\)</span>.</p>
<p>We copied this convention in our implementation of <code class="docutils literal notranslate"><span class="pre">AdamWL2SP</span></code>, though we feel that using the given <span class="math notranslate nohighlight">\(\lambda\)</span> as the actual weight decay may have been preferable.</p>
</div>
</section>
<section id="experiments">
<h3>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">#</a></h3>
<p>Ideally, we would perform cross-validation to find the optimal weight decay hyperparameters <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span> for AdamWL2SP. In the interest of saving time we took a much more crude approach, which nevertheless seemed to show promise initially. First of all, as <span class="math notranslate nohighlight">\(\lambda_2\)</span> corresponds to <span class="math notranslate nohighlight">\(\ltwo\)</span> regularization and our baseline has no regularization, we simply set <span class="math notranslate nohighlight">\(\lambda_2=0\)</span>.</p>
<section id="short-runs">
<h4>Short Runs<a class="headerlink" href="#short-runs" title="Permalink to this headline">#</a></h4>
<p>In order to test the hypothesis from the literature that <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization prevents catastrophic forgetting, we use a high learning rate <span class="math notranslate nohighlight">\(\alpha=4e^{-5}\)</span> which resulted in a large number of failed fine-tuning runs<a class="footnote-reference brackets" href="#cf-hypothesis" id="id13">1</a> in the absence of regularization. We picked a single failed run: shuffle seed 20030, and tried fine-tuning for 3 epochs with varying weight decays <span class="math notranslate nohighlight">\(\lambda_1\)</span>. The result was as hoped: Once <span class="math notranslate nohighlight">\(\lambda_1\)</span> became large enough the fine-tuning run was successful. On the other hand, beyond a certain point the fine-tuning failed again. This is to be expected since in the extreme, <span class="math notranslate nohighlight">\(\lambda_1=1\)</span> is equivalent to freezing all of the source layers.</p>
<p>Based on this one example, we found that <span class="math notranslate nohighlight">\(\lambda_1\in\{0, 2e^{-5}, 2e^{-4}, 2e^{-3}\}\)</span>, or equivalently <span class="math notranslate nohighlight">\(\lambda_1=\alpha\lambda_1'\)</span> for <span class="math notranslate nohighlight">\(\lambda_1'\in\{0.0, 0.5, 5.0, 50.0\}\)</span>, were reasonable hyperparameter values (<span class="math notranslate nohighlight">\(\lambda_1=0\)</span> of course means no regularization). Then the experiment from <a class="reference internal" href="instability.html#short-runs"><span class="std std-ref">Short Runs</span></a> with 40 fine-tuning runs was repeated with <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization for these parameters.</p>
<div class="cell tag_remove-input docutils container">
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
</style>
<table id="T_ff2a6">
  <caption>Accuracy of ALBERT Fine-Tuned on RTE with AdamWL2SP for 3 Epochs at learning rate 4e-5</caption>
  <thead>
    <tr>
      <th class="index_name level0" >penalty:</th>
      <th id="T_ff2a6_level0_col0" class="col_heading level0 col0" >0</th>
      <th id="T_ff2a6_level0_col1" class="col_heading level0 col1" >2e-05</th>
      <th id="T_ff2a6_level0_col2" class="col_heading level0 col2" >0.0002</th>
      <th id="T_ff2a6_level0_col3" class="col_heading level0 col3" >0.002</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_ff2a6_level0_row0" class="row_heading level0 row0" >mean</th>
      <td id="T_ff2a6_row0_col0" class="data row0 col0" >67.1%</td>
      <td id="T_ff2a6_row0_col1" class="data row0 col1" >64.9%</td>
      <td id="T_ff2a6_row0_col2" class="data row0 col2" >67.6%</td>
      <td id="T_ff2a6_row0_col3" class="data row0 col3" >67.5%</td>
    </tr>
    <tr>
      <th id="T_ff2a6_level0_row1" class="row_heading level0 row1" >std</th>
      <td id="T_ff2a6_row1_col0" class="data row1 col0" >9.4%</td>
      <td id="T_ff2a6_row1_col1" class="data row1 col1" >11.7%</td>
      <td id="T_ff2a6_row1_col2" class="data row1 col2" >9.3%</td>
      <td id="T_ff2a6_row1_col3" class="data row1 col3" >9.6%</td>
    </tr>
    <tr>
      <th id="T_ff2a6_level0_row2" class="row_heading level0 row2" >min</th>
      <td id="T_ff2a6_row2_col0" class="data row2 col0" >47.7%</td>
      <td id="T_ff2a6_row2_col1" class="data row2 col1" >47.3%</td>
      <td id="T_ff2a6_row2_col2" class="data row2 col2" >44.8%</td>
      <td id="T_ff2a6_row2_col3" class="data row2 col3" >46.6%</td>
    </tr>
    <tr>
      <th id="T_ff2a6_level0_row3" class="row_heading level0 row3" >max</th>
      <td id="T_ff2a6_row3_col0" class="data row3 col0" >79.1%</td>
      <td id="T_ff2a6_row3_col1" class="data row3 col1" >80.9%</td>
      <td id="T_ff2a6_row3_col2" class="data row3 col2" >78.0%</td>
      <td id="T_ff2a6_row3_col3" class="data row3 col3" >79.4%</td>
    </tr>
    <tr>
      <th id="T_ff2a6_level0_row4" class="row_heading level0 row4" >failed runs</th>
      <td id="T_ff2a6_row4_col0" class="data row4 col0" >8</td>
      <td id="T_ff2a6_row4_col1" class="data row4 col1" >15</td>
      <td id="T_ff2a6_row4_col2" class="data row4 col2" >8</td>
      <td id="T_ff2a6_row4_col3" class="data row4 col3" >8</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Disappointingly, the summary statistics are not significantly different than without regularization, except for the inexplicable fact that the weakest regularization parameter in fact decreased stability.</p>
<p>The full details of the validation set accuracy during fine-tuning are plotted in an attempt to glean more information (click to enlarge).</p>
<div class="full-width docutils">
<a class="reference internal image-reference" href="_images/results.short_runs.l2sp.lr3e-5.png"><img alt="_images/results.short_runs.l2sp.lr3e-5.png" src="_images/results.short_runs.l2sp.lr3e-5.png" style="width: 100%;" /></a>
</div>
<p>Many of the fine-tuning runs are at least individually consistent over the different amount of regularization <span class="math notranslate nohighlight">\(\lambda_1\)</span>. For a number of runs, like shuffle seed 20030, the fine-tuning is only successful with <span class="math notranslate nohighlight">\(\lambda_1'\ge 5.0\)</span>. For seed 20001 it is at least consistent that the highest regularization causes the fine-tuning to fail. At the very least the ideal amount regularization varies between examples, apparently making it too imprecise to be helpful.</p>
<p>However, quite a few runs are not even consistent, such as seed 20021 for example. In many cases it appears that the regularization is behaving more like altering the randomness, i.e. having a similar effect to using a different shuffle seed.</p>
</section>
<section id="long-runs">
<h4>Long Runs<a class="headerlink" href="#long-runs" title="Permalink to this headline">#</a></h4>
<p>As we saw in <a class="reference internal" href="instability.html#instability"><span class="std std-ref">Fine-Tuning Instability</span></a>, fine-tuning for longer runs using the baseline method of <span id="id14">[<a class="reference internal" href="references.html#id9" title="Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. arXiv:2006.04884 [cs, stat], March 2021. arXiv: 2006.04884. URL: http://arxiv.org/abs/2006.04884 (visited on 2022-04-11).">MAK21</a>]</span> was much more effective than “few-shot” short runs. Next we examine the effect of AdamWL2SP regularization upon this baseline. Based on the results presented in <a class="reference internal" href="instability.html#long-runs"><span class="std std-ref">Long Runs</span></a> we try regularizing with a learning rate of <span class="math notranslate nohighlight">\(\alpha=3e^{-5}\)</span>.</p>
<div class="cell tag_remove-input docutils container">
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
</style>
<table id="T_95123">
  <caption>Accuracy of ALBERT Fine-Tuned on RTE with AdamWL2SP for 20 Epochs at learning rate 3e-5</caption>
  <thead>
    <tr>
      <th class="index_name level0" >penalty:</th>
      <th id="T_95123_level0_col0" class="col_heading level0 col0" >0</th>
      <th id="T_95123_level0_col1" class="col_heading level0 col1" >1.5e-05</th>
      <th id="T_95123_level0_col2" class="col_heading level0 col2" >0.00015</th>
      <th id="T_95123_level0_col3" class="col_heading level0 col3" >0.0015</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_95123_level0_row0" class="row_heading level0 row0" >mean</th>
      <td id="T_95123_row0_col0" class="data row0 col0" >78.2%</td>
      <td id="T_95123_row0_col1" class="data row0 col1" >79.3%</td>
      <td id="T_95123_row0_col2" class="data row0 col2" >77.5%</td>
      <td id="T_95123_row0_col3" class="data row0 col3" >78.3%</td>
    </tr>
    <tr>
      <th id="T_95123_level0_row1" class="row_heading level0 row1" >std</th>
      <td id="T_95123_row1_col0" class="data row1 col0" >6.1%</td>
      <td id="T_95123_row1_col1" class="data row1 col1" >4.7%</td>
      <td id="T_95123_row1_col2" class="data row1 col2" >8.4%</td>
      <td id="T_95123_row1_col3" class="data row1 col3" >6.0%</td>
    </tr>
    <tr>
      <th id="T_95123_level0_row2" class="row_heading level0 row2" >min</th>
      <td id="T_95123_row2_col0" class="data row2 col0" >47.7%</td>
      <td id="T_95123_row2_col1" class="data row2 col1" >52.7%</td>
      <td id="T_95123_row2_col2" class="data row2 col2" >47.3%</td>
      <td id="T_95123_row2_col3" class="data row2 col3" >49.5%</td>
    </tr>
    <tr>
      <th id="T_95123_level0_row3" class="row_heading level0 row3" >max</th>
      <td id="T_95123_row3_col0" class="data row3 col0" >83.4%</td>
      <td id="T_95123_row3_col1" class="data row3 col1" >84.5%</td>
      <td id="T_95123_row3_col2" class="data row3 col2" >83.4%</td>
      <td id="T_95123_row3_col3" class="data row3 col3" >84.5%</td>
    </tr>
    <tr>
      <th id="T_95123_level0_row4" class="row_heading level0 row4" >max_path</th>
      <td id="T_95123_row4_col0" class="data row4 col0" >84.5%</td>
      <td id="T_95123_row4_col1" class="data row4 col1" >84.5%</td>
      <td id="T_95123_row4_col2" class="data row4 col2" >84.5%</td>
      <td id="T_95123_row4_col3" class="data row4 col3" >84.5%</td>
    </tr>
    <tr>
      <th id="T_95123_level0_row5" class="row_heading level0 row5" >failed runs</th>
      <td id="T_95123_row5_col0" class="data row5 col0" >1</td>
      <td id="T_95123_row5_col1" class="data row5 col1" >1</td>
      <td id="T_95123_row5_col2" class="data row5 col2" >2</td>
      <td id="T_95123_row5_col3" class="data row5 col3" >1</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>There is some improvement at the weakest regularization <span class="math notranslate nohighlight">\(\lambda_1=0.5 \alpha = 1.5e^{-5}\)</span>. It has the highest average accuracy and lowest standard deviation. It also has a higher maximum accuracy than fine-tuning at <span class="math notranslate nohighlight">\(\alpha=2e^{-5}\)</span>. While this was the best performing setting, we would need to run more tests to determine whether this is a statistically significant effect.</p>
<p>Unfortunately, there is still one failed fine-tuning run contrary to our hopes that <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization would mitigate this.</p>
</section>
<section id="accuracy-vs-epoch-plot">
<h4>Accuracy vs Epoch Plot<a class="headerlink" href="#accuracy-vs-epoch-plot" title="Permalink to this headline">#</a></h4>
<p>The full details of the fine-tuning runs at this best performing setting are plotted below.</p>
<div class="full-width docutils">
<a class="reference internal image-reference" href="_images/results.long_run.l2sp_0.5.lr3e-5.png"><img alt="_images/results.long_run.l2sp_0.5.lr3e-5.png" src="_images/results.long_run.l2sp_0.5.lr3e-5.png" style="width: 100%;" /></a>
</div>
<p>Compared with the fine-tuning run shown in <a class="reference internal" href="instability.html#long-run-plot"><span class="std std-ref">Accuracy vs Epoch Plot</span></a> without regularization, we see some improvement. There is only one failed run (seed 20024), and no other “poor performing run” as in the other. However, our hope (perhaps unrealistic) with <span class="math notranslate nohighlight">\(\ltsp\)</span> regularization was to completely avoid failed runs.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="cf-hypothesis"><span class="brackets"><a class="fn-backref" href="#id13">1</a></span></dt>
<dd><p>Of course we are really testing the hypothesis that the regularization avoids failed fine-tuning runs here. We would need other tests to confirm specifically about the closely related catastrophic forgetting phenomenon.</p>
</dd>
</dl>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="instability.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Fine-Tuning Instability</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="conclusion.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Conclusions</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By James Hirschorn<br/>
  
      &copy; Copyright 2022.<br/>
    <div class="extra_footer">
      <a href="https://quantitative-technologies.com/" style="font-size:150%;">
  <b>Quantitative Technologies</b>
</a>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>